# MCP-Native Architecture Guide

## ğŸ¯ **True MCP Implementation**

This system now uses **100% MCP protocol** for all communications. Everything is an MCP server:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Orchestrator (Router)         â”‚
â”‚   - Thin MCP client                 â”‚
â”‚   - Routes messages between servers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (MCP Protocol)
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“         â†“         â†“
[LLM MCP        [Tools    [Agents   [Future
 Server]         MCP       MCP       MCP
                Server]   Server]   Servers]
```

## ğŸ“ **New Files Created**

### **MCP Servers:**
1. **`mcp_server/llm_mcp_server.py`** - LLM cascade as MCP server
2. **`mcp_server/agents_mcp_server.py`** - Expert agents as MCP server
3. **`mcp_server/unified_server.py`** - Tools as MCP server (already existed)

### **MCP Orchestrator:**
4. **`core/mcp_orchestrator.py`** - Pure MCP router/client

### **New Entry Point:**
5. **`main_mcp.py`** - MCP-native CLI

## ğŸš€ **How to Run**

### **Start the MCP System:**

```bash
python main_mcp.py
```

That's it! The orchestrator will automatically:
1. Start LLM MCP server
2. Start Tools MCP server
3. Start Agents MCP server
4. Connect to all servers via MCP protocol
5. Route messages between them

## ğŸ”„ **How It Works**

### **Message Flow:**

```
1. User: "analyze iris data in ./data"
   â†“
2. Orchestrator â†’ LLM MCP Server
   generate_completion(messages, tools)
   â†“
3. LLM returns: tool_calls = [
     {name: "list_directory", args: {dir_path: "./data"}},
     {name: "load_csv", args: {file_path: "./data/iris.csv"}},
     {name: "analyze_dataframe", args: {df_id: "..."}}
   ]
   â†“
4. Orchestrator routes each tool call:
   - list_directory â†’ Tools MCP Server
   - load_csv â†’ Tools MCP Server
   - analyze_dataframe â†’ Tools MCP Server
   â†“
5. Orchestrator sends results back to LLM MCP Server
   â†“
6. LLM generates final response
   â†“
7. User receives answer
```

### **Key Differences from Old Architecture:**

| Old (Direct Calls) | New (MCP-Native) |
|-------------------|------------------|
| `import file_tools` | MCP client â†’ Tools MCP Server |
| `file_tools.read_file()` | `tools_session.call_tool("read_file")` |
| Direct Python function | MCP protocol message |
| Single process | Multiple MCP server processes |
| Orchestrator decides flow | **LLM decides flow via tool calls** |

## ğŸ› ï¸ **MCP Server Details**

### **1. LLM MCP Server**

**File:** `mcp_server/llm_mcp_server.py`

**Exposes:**
- `generate_completion(messages, tools)` - Generate LLM response with tool calling
- `get_llm_stats()` - Get usage statistics
- `list_available_providers()` - List cascade providers

**How it works:**
- Wraps the LLM cascade (DeepSeek â†’ Gemini â†’ GPT-5 â†’ Claude)
- Exposes as MCP tool
- Returns responses with tool_calls that orchestrator can route

**Start standalone:**
```bash
python mcp_server/llm_mcp_server.py
```

### **2. Tools MCP Server**

**File:** `mcp_server/unified_server.py`

**Exposes:**
- File operations: `read_file`, `write_file`, `list_directory`, `search_files`
- Data analysis: `load_csv`, `analyze_dataframe`, `compute_statistics`
- Visualization: `create_matplotlib_plot`, `create_seaborn_plot`, `save_figure`

**Start standalone:**
```bash
python mcp_server/unified_server.py
```

### **3. Agents MCP Server**

**File:** `mcp_server/agents_mcp_server.py`

**Exposes:**
- `create_cover_image(prompt, style)` - Generate article cover
- `create_abstract_figure(description, style)` - Create graphical abstract
- `process_image(image_path, instruction)` - Process existing image
- `list_available_agents()` - List available agents

**Start standalone:**
```bash
python mcp_server/agents_mcp_server.py
```

## ğŸ® **CLI Commands**

Run `main_mcp.py` and use:

- `/help` - Show help
- `/clear` - Clear conversation history
- `/new` - Start new session
- `/stats` - Show MCP server connection status
- `/mcp` - Show detailed MCP architecture info
- `/exit` - Exit (closes all MCP connections)

## ğŸ” **LLM-Driven Flow**

**The key difference:** The LLM now drives the entire workflow!

### **Example:**

```
User: "analyze iris data and create a scatter plot"

LLM MCP Server receives messages and returns:
{
  "content": "I'll analyze the iris data for you.",
  "tool_calls": [
    {
      "name": "search_files",
      "arguments": {"root_dir": "./data", "pattern": "*.csv"}
    }
  ]
}

Orchestrator routes to Tools MCP Server â†’
Returns: ["./data/iris.csv"]

Orchestrator sends result back to LLM MCP Server â†’

LLM returns:
{
  "tool_calls": [
    {
      "name": "load_csv",
      "arguments": {"file_path": "./data/iris.csv"}
    }
  ]
}

... and so on until LLM is satisfied and returns final response.
```

## ğŸ“Š **Benefits of MCP Architecture**

âœ… **True Separation:** Each component is independent MCP server
âœ… **LLM-Driven:** LLM decides what to do next, not hardcoded logic
âœ… **Scalable:** Can run MCP servers on different machines
âœ… **Extensible:** Add new MCP servers without changing orchestrator
âœ… **Standard Protocol:** Uses industry-standard MCP
âœ… **Distributable:** MCP servers can be anywhere
âœ… **Tool Discovery:** Orchestrator queries servers for available tools
âœ… **Isolation:** Each server runs in its own process

## ğŸ”§ **Debugging**

### **Check MCP Server Status:**

Use `/stats` command in CLI to see connection status.

### **Run Servers Manually:**

For debugging, run each server separately:

```bash
# Terminal 1
python mcp_server/llm_mcp_server.py

# Terminal 2
python mcp_server/unified_server.py

# Terminal 3
python mcp_server/agents_mcp_server.py

# Terminal 4
python main_mcp.py
```

### **Check Logs:**

Set `LOG_LEVEL=DEBUG` in `.env` for detailed MCP communication logs.

## ğŸ¯ **Next Steps**

Now that you have true MCP architecture:

1. âœ… **Test the system:** Run `python main_mcp.py`
2. âœ… **Try data analysis:** "analyze iris data in ./data"
3. âœ… **Try visualization:** "create a scatter plot"
4. âœ… **Try image generation:** "create a cover image"
5. âœ… **Explore:** Use `/mcp` to see architecture details

## ğŸ†š **Old vs New**

### **Old (`main.py`):**
- Direct Python imports
- Orchestrator calls functions directly
- Single process
- Fast but not MCP-compliant

### **New (`main_mcp.py`):**
- MCP protocol everywhere
- LLM drives the flow
- Multiple server processes
- True MCP implementation

Both are available - use `main_mcp.py` for true MCP architecture!

## ğŸ‰ **You Now Have:**

âœ… **100% MCP-native architecture**
âœ… **LLM-driven workflow** (LLM decides what to do)
âœ… **Distributed capable** (servers can run anywhere)
âœ… **Standard protocol** (industry-standard MCP)
âœ… **Fully extensible** (add MCP servers easily)

**This is what you asked for - everything via MCP! ğŸš€**
